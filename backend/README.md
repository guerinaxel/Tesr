# AI Code Assistant â€“ Backend (Django + RAG + Ollama)

Ce backend fournit une API permettant dâ€™interroger votre base de code (Python, Django, Angularâ€¦)
Ã  lâ€™aide dâ€™un modÃ¨le LLaMA 3.1 exÃ©cutÃ© via **Ollama**.  
Le modÃ¨le gÃ©nÃ¨re ses rÃ©ponses en utilisant la technique **RAG (Retrieval-Augmented Generation)** :
votre code est indexÃ©, vectorisÃ©, puis utilisÃ© comme contexte pertinent pour chaque requÃªte.

---

## âœ¨ FonctionnalitÃ©s

- Extraction automatique du code du projet (backend + frontend).
- DÃ©coupage intelligent en chunks pour du contexte prÃ©cis.
- Vectorisation via `nomic-ai/nomic-embed-text-v1.5` (Sentence Transformers).
- Index FAISS rapide et persistant (`rag_index.faiss`).
- API REST `/api/code-qa/` pour poser des questions sur le code.
- IntÃ©gration Ollama + LLaMA 3.1 locale.
- Commande Django : `build_rag_index` pour reconstruire lâ€™index.

---

## ğŸ“ Structure du backend

```
backend/
â”œâ”€â”€ project/
â”œâ”€â”€ codeqa/
â”‚   â”œâ”€â”€ rag_index.py
â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”œâ”€â”€ views.py
â”‚   â”œâ”€â”€ serializers.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â””â”€â”€ management/
â”‚        â””â”€â”€ commands/
â”‚              â””â”€â”€ build_rag_index.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

---

## ğŸ“¦ DÃ©pendances

### Python
- Django 5
- Django REST Framework
- sentence-transformers
- sentencepiece (requis par `nomic-ai/nomic-embed-text-v1.5`)
- Nomic client (pour les embeddings `nomic-embed-*`)
- FAISS CPU
- Ollama Python client
- python-dotenv

Installables via :

```bash
pip install -r requirements.txt
```

### SystÃ¨me
- Docker (optionnel mais recommandÃ©)
- Ollama installÃ© localement  
  â†’ https://ollama.com/download

---

## ğŸ› ï¸ Installation

### 1. Cloner le projet

```bash
git clone <votre-repo>
cd backend
```

### 2. CrÃ©er un environnement Python

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. Configurer Ollama + LLaMA

```bash
ollama pull llama3.1:8b
```

### 5. Configurer lâ€™environnement

CrÃ©er `.env` :

```
DJANGO_DEBUG=True
OPENAI_API_KEY=  # vide si pas utilisÃ©
```

### 6. Appliquer les migrations Django

```bash
python manage.py migrate
```

### 7. Construire lâ€™index RAG

```bash
python manage.py build_rag_index
```

Cela gÃ©nÃ¨re :

```
rag_index.faiss
rag_docs.pkl
```

---

## â–¶ï¸ Lancement du serveur

### Option A â€” Python local

```bash
python manage.py runserver
```

### Option B â€” Docker (backend + Ollama)

```bash
docker compose up --build
```

---

## ğŸ¤– Utilisation de lâ€™API

### Endpoint : `POST /api/code-qa/`

#### Exemple de requÃªte :

```json
{
  "question": "Ã€ quoi sert le fichier models.py dans lâ€™app accounts ?"
}
```

#### Exemple de rÃ©ponse :

```json
{
  "answer": "Le fichier models.py dÃ©finit les modÃ¨les ORM..."
}
```

---

## ğŸ§© Personnalisation

- Ajouter ou exclure certaines extensions â†’ `ALLOWED_EXT` dans `rag_index.py`
- Modifier le modÃ¨le dâ€™embedding â†’ variable dâ€™environnement `RAG_EMBED_MODEL` (par dÃ©faut `nomic-ai/nomic-embed-text-v1.5`).
- DÃ©finir un modÃ¨le de secours en cas dâ€™Ã©chec de tÃ©lÃ©chargement â†’ `RAG_EMBED_MODEL_FALLBACK` (par dÃ©faut `sentence-transformers/all-MiniLM-L6-v2`).
- Le backend reconstruit automatiquement lâ€™index FAISS si la dimension des embeddings change (ex. passage dâ€™un ancien modÃ¨le vers Nomic).
- Augmenter la profondeur RAG â†’ `k=5` â†’ `k=10`

---

## âœ… Tests & couverture

Lancer les tests avec la couverture obligatoire (â‰¥ 80 %) :

```bash
python manage.py test
```

Les rapports XML JUnit et `coverage.xml` sont gÃ©nÃ©rÃ©s dans `test-results/` et le build Ã©choue automatiquement si le seuil est franchi.

---

## ğŸ›¡ï¸ SÃ©curitÃ©

- Pas dâ€™accÃ¨s au systÃ¨me de fichiers via lâ€™API.
- Index gÃ©nÃ©rÃ© une fois, non reconstruit Ã  chaque requÃªte.
- Aucune donnÃ©e envoyÃ©e Ã  des services externes (exÃ©cution 100% locale).

---

## ğŸ“„ Licence

MIT (modifiable selon votre projet).
